<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->


    <title>Ryan's Data Blog</title>
    <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-57002012-5', 'auto');
          ga('send', 'pageview');

        </script>
  </head>

  <body>
    <header>
      <div class="inner">
        <h1>Ryan's Data Blog</h1>
        <h2 style='color:white'>
          <script type="text/javascript">
            var s1 = "slootryan";
            var g2 = "@";
            var j3 = "gmail.com";
            var buffalo = s1 + ' at ' + 'gmail '+ 'dot com';
            document.write("<a href=" + "mail" + "to:" + s1 + g2 + j3 + " style='color:white'>" + buffalo + "</a>");
          </script>
        </h2>
        <a href="https://github.com/rsloot" class="button"><small>Follow me on</small> GitHub</a>
        <a href="https://twitter.com/r_sloot" class="twitter-follow-button" data-show-count="false">Follow on twitter</a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script><br>
        <script src="//platform.linkedin.com/in.js" type="text/javascript"></script>
        <script type="IN/MemberProfile" data-id="https://www.linkedin.com/in/rsloot" data-format="click" data-related="false" data-text="Ryan Sloot" style='color:white'></script>
      </div>

    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">

<h3>
<a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>**Robot Localization**</h3>
<p>
<!-- ###Robot Localization -->
Get and predict robot location, given observation values from a sensor.<br><br>
<!-- <strong>What I did:</strong><br> -->
Fun project! A ton of trial and error with the initial algorithms! Here is what is being done in the program: Given observation from noisy sensor, try to locate and estimate the robot's hidden location. The sensor provides a distribution of probable values. The robot is equally likely to be in any of the locations on the grid, initially. The robot has a transition model with a distribution, which is given in <a href="https://github.com/rsloot/Computational-Probability-and-Inference/blob/master/robot/robot.py">robot.py</a> file. Treating the distribution of hidden states as a hidden markov model (HMM); I implement forward-backward algorithm, by getting distribution of current observation (phi) with the previous message--the state from previous movement; prior distribution on initial step--then get sum of the distribution of the transition model for each of the possible values from weights (phi*previous message). This becomes the message for the next stage, iterate for all observations, which gets us the forward messages. The process for backward messages is similar, using the transpose--or columns--of the transition distribution matrix. Once we have the forward and backward messages, we again loop through the observations calculating the marginals for each observation, multiplying the messages--forward times backward--by the distribution for each observation in the set of given observations (backward_message[i] * forward_message[i] * phi (observations distribution) ). Next I implemented and calculate the MAP estimation for HMMs using the Viterbi algorithm. To implement Viterbi I used Min-Sum for HMMs, where each message is the minimum of the sum of the $-log_2$ of the previous message, phi (observation), and psi (transition), storing the argmin for each traceback messages needed for viterbi. Once we get the minimum over all possible states, we get the argmin for the last last message, and using our traceback messages we  get the MAP estimate for Viterbi.</p>
<p>
  The struggle was trying to implement forward-backward using dictionaries rather than numpy arrays. I eventually got a forward-backward implementation working, once I got to Viterbi I knew it would be easier to work with matrices. I reworked forward-backward using matrices, which made implementation rather simple. Using log-scale is a must when dealing with the probabilities, even in this limited space, underflow was an issue I quickly realized. I worked two pet examples of both viterbi and forward-backward which help with intuition and simplify bigger project. This is not implemented in a notebook, similar to last project, I will try to put together something that renders nicely to follow along with some screenshots or videos of the robot graphics, which shows estimations as the robot moves around the grid. Next project is a spam/ham email classifier. Classic ML project, but will be implementing from scratch, similar to recommender and localization. <a href="https://github.com/rsloot/Computational-Probability-and-Inference/blob/master/robot">Robot Localization repo.</a>
</p>




<h3>
<a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>A working movie recommender!</h3>
<p>
<!-- Movie Recommendation -->
Predict the rating of a new (unseen) movie, X, given set of ratings for movies seen.<br><br>
<strong>How it's done:</strong>
 Putting information gained and entropy to use; given ratings for a set a movies recommends, compute the posterior distribution, find likelihood of a distribution of the given observation (movie rating). Once we have the likelihood distribution, we use inference (bayes) to get a distributions of posteriors for each possible rating--and its likelihood--for each movie in the dataset. In addition we provide a MAP (Maximum a posteriori) estimate for each movie, which is really what would be shown to an end user; the distribution is interesting though. Once we have the distribution of possible ratings for each movie, implement an entropy function; which gives us the entropy (randomness) for each of the posterior values. <a href="https://github.com/rsloot/Computational-Probability-and-Inference/tree/master/movie-rating">Movie recommender repo.</a></p>
 <p>Did not work this one in a notebook, so posted is not as easily read as the notebooks posted earlier. However I will work to convert into a notebook with outputs that explain the steps taken with screen shots from the run program. Next up is robot localization; cool stuff coming here! Localization is the foundation to self driving cars: getting observations from sensors to predict location of car relative to objects seen. Stay tuned.</p>

<h3>
<a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Probability!</h3>

<p>
 This week was filled with probability! With the movie recommendation project in the works re-affirming Bayesian Inference was a must, although some of <a href="/code/python/Bayes Inference, Expectation, Randomness">this notebook</a> (Bayes Inference...) was a bit of review, I'm not at a point where probability, especially conditioning on multiple events is super intuitive, but python definitely helps to gain more insight. Expectations are weighted averages, not a difficult topic, the linearity and total expectation properties lead to more difficult problems becoming simple; variance should always be calculated using expectations. Have been here before with random variables, the real meat of probability! Solving (note in notebooks) problems similar to hat problem, coupon collector, really only using discrete r.v.s and pmfs in this notebook.</p>
  <p>
    Getting into <a href="/code/python/Measures of Randomness">Measuring Randomness</a> I explore what will really contribute to the movie recommender. Using entropy to measure the information gained in each set of ratings, and divergence to compare the inherent values to the predicted values. In the notebook <a href="/code/python/Measures of Randomness">Measuring Randomness</a> I go through calculating entropy, divergence, mutual information, and play a bit geometric distributions. With all three of these measurements being rooted in machine learning (i.e.decision trees, information gain), it is great to gain some real practice with using entropy and divergence at a lower level than simply looking at scikit-learn docs! Information theory is a fascinating subject, and measuring randomness is really the foundation to the entire field. 
  </p>
  <p>
    I may upload so older data explorations, though some are pretty rough and all over the place. Movie recommender mini project is basically done will upload to github once finished. I've been looking into the <a href="https://lagunita.stanford.edu/courses/course-v1:Engineering+QMSE01+Fall2016/about" style='color:#930000;'>Quantum Mechanics</a> course on Stanford Online, it has been great so far, highly recommended. Also Mining Massive Datasets is about to open up, I may venture into that. Next (new) post will include more probability, using undirected graphical models as data structures for probability distributions. Next project is robot localization!


  </p>

<h3>
<a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>First Post</h3>

<p>Welcome! I will be using this blog to post data reports, explorations, and tutorials. 
Links along the right side and should render html, code available to be downloaded in github repo.
I will post summaries for new explorations with data sources, hoping to get at least a couple in each week, stay tuned. Any
questions or comments email me: slootryan at gmail dot com</p><br>
<p>The data sets already explored include a couple Spark notebooks, where using databricks I explored word counts in Shakespeare's work and log data using Apache Spark, it's crazy how fast using Spark is compared to trying to do this same project on my machine! Both were basically not possible. The R markdowns include a couple of psets from Udacity's Data Analysis course (L4, L5, L6), the other two were tangents while taking the course. Exploring the classic Diamonds dataset, focusing on a bunch of visualization. The Global food supply was an analysis of the global food prduction per person--the (most complete) newest data I could get was from 2007--saw some interesting trends in the DRC, US, and globally. I delved into some more advanced visualizations in the food supply analysis with Plotly.</p>
<p>I have a few more, that I may upload, that are not yet complete. New posts coming should be more interesting, and will include mathmatical sets (i.e. probabalistic) and more machine learning.</p>


<!-- <h3>
<a id="creating-pages-manually" class="anchor" href="#creating-pages-manually" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Creating pages manually</h3>

<p>If you prefer to not use the automatic generator, push a branch named <code>gh-pages</code> to your repository to create a page manually. In addition to supporting regular HTML content, GitHub Pages support Jekyll, a simple, blog aware static site generator. Jekyll makes it easy to create site-wide headers and footers without having to copy them across every page. It also offers intelligent blog support and other advanced templating features.</p>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Authors and Contributors</h3>

<p>You can @mention a GitHub username to generate a link to their profile. The resulting <code>&lt;a&gt;</code> element will link to the contributor’s GitHub Profile. For example: In 2007, Chris Wanstrath (<a href="https://github.com/defunkt" class="user-mention">@defunkt</a>), PJ Hyett (<a href="https://github.com/pjhyett" class="user-mention">@pjhyett</a>), and Tom Preston-Werner (<a href="https://github.com/mojombo" class="user-mention">@mojombo</a>) founded GitHub.</p>

<h3>
<a id="support-or-contact" class="anchor" href="#support-or-contact" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Support or Contact</h3>

<p>Having trouble with Pages? Check out our <a href="https://help.github.com/pages">documentation</a> or <a href="https://github.com/contact">contact support</a> and we’ll help you sort it out.</p> -->
        </section>

        <aside id="sidebar">


	<details open>
  			<summary><strong>Data Analysis (R Markdown)</strong></summary>
          <ul>
            <li> <a href="/code/r/rl4">L4</a> </li>
            <li> <a href="/code/r/rl5">L5</a> </li>
            <li> <a href="/code/r/rl6">L6</a> </li>
            <li> <a href="/code/r/food_supply">Global food supply (2007)</a> </li>
            <li> <a href="/code/r/many_vars">Diamonds</a> </li>
          </ul>
    </details>

    <details open>
          <summary><strong>Spark Notebooks</strong></summary>
          <ul>
            <li> <a href="/code/python/word_count">Word Count</a></li>
            <li> <a href="/code/python/apache_log">Apache Server Log Analysis</a></li>
          </ul>
    </details>

	<details open>
          <summary><strong>Probability (Python)</strong></summary>
          <ul>
          	<li><a href="/code/python/Computational_Probability(0)">Computational Probability(0)</a></li>
            <li><a href="/code/python/Bayes Inference, Expectation, Randomness">Bayes Inference, Expectation, Randomness</a></li>
            <li><a href="/code/python/Measures of Randomness">Measuring Randomness</a></li>
          </ul>
    </details>


          <!-- <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the Architect theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.</p> -->
        </aside>
      </div>
    </div>

  
  </body>


  <script async src="//platform.twitter.com/widgets.js" charset="utf-8">
    <a href="https://twitter.com/TwitterDev" class="twitter-follow-button" data-show-count="false">Follow @TwitterDev</a>

  </script>
</html>
